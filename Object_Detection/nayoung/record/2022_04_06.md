# multi scale test

- ì°¸ê³  : [mmdet.datasets.pipelines.test_time_aug â€” MMDetection 2.23.0 documentation](https://mmdetection.readthedocs.io/en/latest/_modules/mmdet/datasets/pipelines/test_time_aug.html)

```dart
# Copyright (c) OpenMMLab. All rights reserved.
import warnings

import mmcv

from ..builder import PIPELINES
from .compose import Compose

[docs]@PIPELINES.register_module()
class MultiScaleFlipAug:
    """Test-time augmentation with multiple scales and flipping.

    An example configuration is as followed:

    .. code-block::

        img_scale=[(1333, 400), (1333, 800)],
        flip=True,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(type='Normalize', **img_norm_cfg),
            dict(type='Pad', size_divisor=32),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img']),
        ]

    After MultiScaleFLipAug with above configuration, the results are wrapped
    into lists of the same length as followed:

    .. code-block::

        dict(
            img=[...],
            img_shape=[...],
            scale=[(1333, 400), (1333, 400), (1333, 800), (1333, 800)]
            flip=[False, True, False, True]
            ...
        )

    Args:
        transforms (list[dict]): Transforms to apply in each augmentation.
        img_scale (tuple | list[tuple] | None): Images scales for resizing.
        scale_factor (float | list[float] | None): Scale factors for resizing.
        flip (bool): Whether apply flip augmentation. Default: False.
        flip_direction (str | list[str]): Flip augmentation directions,
            options are "horizontal", "vertical" and "diagonal". If
            flip_direction is a list, multiple flip augmentations will be
            applied. It has no effect when flip == False. Default:
            "horizontal".
    """

    def __init__(self,
                 transforms,
                 img_scale=None,
                 scale_factor=None,
                 flip=False,
                 flip_direction='horizontal'):
        self.transforms = Compose(transforms)
        assert (img_scale is None) ^ (scale_factor is None), (
            'Must have but only one variable can be set')
        if img_scale is not None:
            self.img_scale = img_scale if isinstance(img_scale,
                                                     list) else [img_scale]
            self.scale_key = 'scale'
            assert mmcv.is_list_of(self.img_scale, tuple)
        else:
            self.img_scale = scale_factor if isinstance(
                scale_factor, list) else [scale_factor]
            self.scale_key = 'scale_factor'

        self.flip = flip
        self.flip_direction = flip_direction if isinstance(
            flip_direction, list) else [flip_direction]
        assert mmcv.is_list_of(self.flip_direction, str)
        if not self.flip and self.flip_direction != ['horizontal']:
            warnings.warn(
                'flip_direction has no effect when flip is set to False')
        if (self.flip
                and not any([t['type'] == 'RandomFlip' for t in transforms])):
            warnings.warn(
                'flip has no effect when RandomFlip is not in transforms')

    def __call__(self, results):
        """Call function to apply test time augment transforms on results.

        Args:
            results (dict): Result dict contains the data to transform.

        Returns:
           dict[str: list]: The augmented data, where each value is wrapped
               into a list.
        """

        aug_data = []
        flip_args = [(False, None)]
        if self.flip:
            flip_args += [(True, direction)
                          for direction in self.flip_direction]

        for scale in self.img_scale: 
            for flip, direction in flip_args:
                _results = results.copy()
                _results[self.scale_key] = scale
                _results['flip'] = flip
                _results['flip_direction'] = direction
                data = self.transforms(_results)
                aug_data.append(data)

        # list of dict to dict of list
        aug_data_dict = {key: [] for key in aug_data[0]}
        for data in aug_data:
            for key, val in data.items():
                aug_data_dict[key].append(val)
        return aug_data_dict 

    def __repr__(self):
        repr_str = self.__class__.__name__
        repr_str += f'(transforms={self.transforms}, '
        repr_str += f'img_scale={self.img_scale}, flip={self.flip}, '
        repr_str += f'flip_direction={self.flip_direction})'
        return repr_str
```

# EfficientDet Transform

```python
# ResizePad # annotation 
def __call__(self, img, anno: dict):
...

	if 'bbox' in anno:
	            bbox = anno['bbox']
	            bbox[:, :4] *= img_scale

	            bbox_bound = (min(scaled_h, self.target_size[0]), min(scaled_w, self.target_size[1]))
	            clip_boxes_(bbox, bbox_bound)  # crop to bounds of target image or letter-box, whichever is smaller
	            valid_indices = (bbox[:, :2] < bbox[:, 2:4]).all(axis=1)
	            anno['bbox'] = bbox[valid_indices, :]
	            anno['cls'] = anno['cls'][valid_indices]
	
	        anno['img_scale'] = 1. / img_scale  # back to original
	        return new_img, anno
	
```

```python
# RandomResizePad # annotation

def _get_params(self, img):
  # Select a random scale factor.
  scale_factor = random.uniform(*self.scale)
  scaled_target_height = scale_factor * self.target_size[0]
  scaled_target_width = scale_factor * self.target_size[1]

  # Recompute the accurate scale_factor using rounded scaled image size.
  width, height = img.size
  img_scale_y = scaled_target_height / height
  img_scale_x = scaled_target_width / width
	img_scale = min(img_scale_y, img_scale_x)

  # Select non-zero random offset (x, y) if scaled image is larger than target size
  scaled_h = int(height * img_scale)
  scaled_w = int(width * img_scale)

	offset_y = scaled_h - self.target_size[0]
	offset_x = scaled_w - self.target_size[1]

	offset_y = int(max(0.0, float(offset_y)) * random.uniform(0, 1))
	offset_x = int(max(0.0, float(offset_x)) * random.uniform(0, 1))

	return scaled_h, scaled_w, offset_y, offset_x, img_scale
	

def __call__(self, img, anno: dict):
...
	scaled_h, scaled_w, offset_y, offset_x, img_scale = self._get_params(img)

	if 'bbox' in anno:
	            bbox = anno['bbox']  # for convenience, modifies in-place
	            bbox[:, :4] *= img_scale

	            box_offset = np.stack([offset_y, offset_x] * 2)
	            bbox -= box_offset

	            bbox_bound = (min(scaled_h, self.target_size[0]), min(scaled_w, self.target_size[1]))
	            clip_boxes_(bbox, bbox_bound)  # crop to bounds of target image or letter-box, whichever is smaller
	            valid_indices = (bbox[:, :2] < bbox[:, 2:4]).all(axis=1)
	            anno['bbox'] = bbox[valid_indices, :]
	            anno['cls'] = anno['cls'][valid_indices]
	
	        anno['img_scale'] = 1. / img_scale  # back to original
	        return new_img, anno
```

```python
# ResizePad # RandomResizePad # annotation ë³€í˜• ë¶€ë¶„ ì°¨ì´

# 2) RandomResizePadì—ë§Œ ì¡´ìž¬ 
box_offset = np.stack([offset_y, offset_x] * 2)
bbox -= box_offset
```

# mmdetection

### MultiScale

- ì°¸ê³  : [mmdet.apis â€” MMDetection 2.23.0 documentation](https://mmdetection.readthedocs.io/en/latest/api.html?highlight=MultiScaleFlipAug#mmdet.datasets.pipelines.MultiScaleFlipAug)
- ì°¸ê³  : [mmdet.datasets.pipelines.test_time_aug â€” MMDetection 2.23.0 documentation](https://mmdetection.readthedocs.io/en/latest/_modules/mmdet/datasets/pipelines/test_time_aug.html#MultiScaleFlipAug)

### Collect

- ì°¸ê³  : [mmdet.apis â€” MMDetection 2.23.0 documentation](https://mmdetection.readthedocs.io/en/latest/api.html?highlight=collect#mmdet.datasets.pipelines.Collect)
- ì°¸ê³  : [mmdet.datasets.pipelines.formatting â€” MMDetection 2.23.0 documentation](https://mmdetection.readthedocs.io/en/latest/_modules/mmdet/datasets/pipelines/formatting.html#Collect)

### pipeline

- ì°¸ê³  : [ê³µë¶€í•˜ë©° ì •ë¦¬í•´ë³´ëŠ” MMDetection íŠœí† ë¦¬ì–¼ ðŸ¤– (1) - Config, Dataset, Data Pipelines (tistory.com)](https://comlini8-8.tistory.com/85)

# albumentations bbox

- ì°¸ê³  : [Using Albumentations to augment bounding boxes for object detection tasks - Albumentations Documentation](https://albumentations.ai/docs/examples/example_bboxes/)
    
    **Define an augmentation pipeline**
    
    To make an augmentation pipeline that works with bounding boxes, you need to pass an instance ofÂ `BboxParams`Â toÂ `Compose`. InÂ `BboxParams`Â you need to specify the format of coordinates for bounding boxes and optionally a few other parameters. For the detailed description ofÂ `BboxParams`Â please refer to the documentation article about bounding boxes -Â [https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/).
    
    ```python
    transform = A.Compose(
        [A.HorizontalFlip(p=0.5)],
        bbox_params=A.BboxParams(format='pascal_voc', label_fields=['category_ids']),
    )
    
    # format : coco / yolo / pascal_voc / albumentations
    # ì°¸ê³  : https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/
    ```
    
    ```python
    def get_valid_transforms():
        return A.Compose(
            [
                A.Resize(height=512, width=512, p=1.0),
                ToTensorV2(p=1.0),
            ], 
            p=1.0, 
            bbox_params=A.BboxParams(
                format='pascal_voc',
                min_area=0, 
                min_visibility=0,
                label_fields=['labels']
            )
        )

    ```
# inference

```python
python validate.py /opt/ml/detection/dataset --model tf_efficientdet_d4_ap --dataset coco --split test --num-gpu 1 -b 4 --checkpoint /opt/ml/detection/baseline/efficientdet-pytorch/output/train/20220405-112836-tf_efficientdet_d4_ap/model_best.pth.tar --num-classes 10 --results /opt/ml/detection/baseline/efficientdet-pytorch/result.json
```

# ì—ëŸ¬

- ì°¸ê³  : [ìƒìœ„, í•˜ìœ„, ë™ì¼ í´ë”ë‚´ì˜ ëª¨ë“ˆ importí•˜ëŠ” ë²• :: ê°€ì¹˜ê´€ì œìž‘ì†Œ (tistory.com)](https://valuefactory.tistory.com/525)
- ì°¸ê³  : [Python ìˆœì—´, ì¡°í•©, product - itertools (velog.io)](https://velog.io/@davkim1030/Python-%EC%88%9C%EC%97%B4-%EC%A1%B0%ED%95%A9-product-itertools)

# TTA ê´€ë ¨ íˆ´ (ë‚˜ì¤‘ì— ì°¸ê³ )

- ì°¸ê³  : [Test Time Augmentation(TTA) (tistory.com)](https://visionhong.tistory.com/26)
- ì°¸ê³  : https://github.com/qubvel/ttach
- ì°¸ê³  : https://github.com/kentaroy47/ODA-Object-Detection-ttA

# mmdetectionì— ìƒˆë¡œìš´ backbone model ë“±ë¡ 
```python
/opt/conda/envs/swin/lib/python3.7/site-packages/mmdet-2.11.0-py3.7.egg/mmdet/models/backbones/__init__.py

ì—¬ê¸°ì— backbone ë‚´ìš© ì¶”ê°€

í•´ë‹¹ í´ë”ì— efficientnet ê´€ë ¨ íŒŒì¼ ì¶”ê°€í•˜ê¸° 

```

### mmdetection efficientdet

- ì°¸ê³  : https://github.com/open-mmlab/mmdetection

# CenterNet

- ì°¸ê³  : https://github.com/xingyizhou/CenterNet

# Object detection

- ì°¸ê³  : [COCO test-dev Benchmark (Object Detection) | Papers With Code](https://paperswithcode.com/sota/object-detection-on-coco)

# Res2Net checkpoint

- ì°¸ê³  : https://github.com/Res2Net/Res2Net-PretrainedModels
