# 모델 성능 향상을 위해서 할 일

- [ ]  augmentation 찾고 리스트
- [ ]  Flip + 내부의 bbox에 있는 tag를 mirrored 하는 특수 처리 → custom augmentation
- [ ]  TTA code 짜기 (multi scale)

### train

```jsx
pip install -r requirements.txt
apt-get install libgl1-mesa-glx

python train.py
```

- 1 epoch 당 Elapsed time: 0:00:50.051758 초 (batch size 12 기준)


### ssh + vscode 연결 

[(3기) AI Stages User Guide_ver1.0.pdf](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9a96bdb6-856b-419e-b519-b940cdd1b4c6/(3기)_AI_Stages_User_Guide_ver1.0.pdf)

- mac의 경우, key 파일의 접근 권한 변경 필요

```jsx
# 터미널에서

chmod 400 [key파일경로]
```

# 기본 실험 준비를 위해 할 일

- [ ]  Optimizer 변경
    - torch optim으로 변경시키는 듯
    - 참고 : [torch.optim — PyTorch 1.11.0 documentation](https://pytorch.org/docs/stable/optim.html)
- [ ]  Scheduler 변경
    - torch optim으로 변경시키는 듯
    - 참고 : [[PyTorch] PyTorch가 제공하는 Learning rate scheduler 정리 (tistory.com)](https://sanghyu.tistory.com/113)
    - 참고 : [torch.optim — PyTorch 1.11.0 documentation](https://pytorch.org/docs/stable/optim.html)
- [ ]  Cross Validation 변경
- [ ]  wandb 설치
- [ ]  loss 변경 가능 ?
- [ ]  best 성능지표로 저장하도록 코드 변경

# Optimizer 변경

### 실험1

- batch size : 12
- epoch : 200
- optimizer : Adam
- lr_scheduler : MultiStepLR
- latest pth
    
    [latest.pth](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/f56b2f92-6211-41af-a439-e7b013d885f2/latest.pth)
    

### 실험2

- batch size : 12
- epoch : 200
- optimizer : SGD
- lr_scheduler : MultiStepLR
- latest pth
