# 할 일

- [ ]  augmentation 빌드 + 적용 + 실험
    
    [https://github.com/Canjie-Luo/Text-Image-Augmentation](https://github.com/Canjie-Luo/Text-Image-Augmentation)
    

# aug

- color + p=0.5

```jsx
wandb: Run summary:
wandb: Angle loss 0.02066
wandb:   Cls loss 0.14655
wandb:   IoU loss 0.40374
wandb:      epoch 200
wandb:       loss 0.57094
wandb:         lr 1e-05
```

# text augmentation

```jsx
# requirements 

GCC 4.8.*
Python 2.7.*
Boost 1.67
OpenCV 2.4.*
```

```jsx
# 루트에서
conda create -n aug python=2.7
conda init --all
source ./.zshrc
conda activate aug

# opt/ml 에서 
git clone https://github.com/Canjie-Luo/Text-Image-Augmentation.git
git checkout master

conda install pip --yes
sudo sudo apt-get install python-numpy # 에러 발생할 경우, 아래의 (1) cell 참고 
pip install numpy
conda install cmake --yes
conda install boost=1.67.0 --yes
conda install --channel https://conda.anaconda.org/menpo opencv --yes # 버전 확인 ?

echo ${PATH} # 용도 경로?

cd Text-Image-Augmentation
mkdir build
cd build
pwd # 용도 경로?

cmake -D CUDA_USE_STATIC_CUDA_RUNTIME=OFF ..
make 

cp Augment.so .. # copy
cd ..
python demo.py # 에러 발생할 경우, 아래의 (2) cell 참고
```

```jsx
(1) sudo sudo apt-get install python-numpy에서 bash: sudo: command not found 에러 발생 
apt-get update && apt-get -y install sudo
```

```jsx
(2) ImportError: libgtk-x11-2.0.so.0: cannot open shared object file: No such file or directory 에러 발생
apt-get update 
apt-get install libgtk2.0-dev # 에러 발생할 경우, 아래의 (3) cell 참고 
```

```jsx
(3) E: Sub-process /usr/bin/dpkg returned an error code (1)
sudo rm /var/lib/dpkg/info/*
sudo dpkg --configure -a
sudo apt update -y

python demo.py # 에러 발생할 경우, 아래의 (4) cell 참고 
```

```jsx
(4) ImportError: /opt/ml/Text-Image-Augmentation/Augment.so: undefined symbol: _ZN2cv6formatB5cxx11EPKcz

```

# **bash: sudo: command not found**

- 참고 : [Docker - bash: sudo: command not found (tistory.com)](https://typo.tistory.com/entry/Docker-bash-sudo-command-not-found)

# ImportError: libgtk-x11-2.0.so.0: cannot open shared object file: No such file or directory

# ERROR: Command errored out with exit status 1:

- 참고 : [파이썬 'Command errored~'오류 해결법 (brunch.co.kr)](https://brunch.co.kr/@198012o8/20)

```jsx
# conda의 python 버전이 낮기 때문에 발생 
conda install python=3.8 # pip install -U pip 으로는 해결 안 됨
```

# 가상환경

```jsx
conda remove --name aug --all
```

# ** No rule to make target '/opt/conda/envs/aug/lib/libboost_python38.so.1.73.0', needed by '[Augment.so](http://augment.so/)'. Stop.

# E: Could not get lock /var/lib/dpkg/lock frontend - open (11: Resource temporarily unavailable)

# E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), is another process using it?

```jsx
sudo killall apt apt-get

  - 진행중인 프로세스가 없다라고 뜨면, 아래와 같이 하나하나씩 디렉토리를 삭제해주세요.

sudo rm /var/lib/apt/lists/lock

sudo rm /var/cache/apt/archives/lock

sudo rm /var/lib/dpkg/lock*

sudo dpkg --configure -a 

sudo apt update

출처: https://enant.tistory.com/18 [ENAN]
```

# 깃허브

- 끝난 branch는 항상 삭제하기

```jsx
git branch -D [branch 이름]
```

# dataset

- 참고 : [100 People - Handwriting OCR Data of Japanese and Korean_Data Products_Datatang](https://datatang.ai/datasets/127)

# augmentation

- 참고 : [Belval/TextRecognitionDataGenerator: A synthetic data generator for text recognition (github.com)](https://github.com/Belval/TextRecognitionDataGenerator)

# aug + Color

```jsx
wandb: Run summary:
wandb: Angle loss 0.01308
wandb:   Cls loss 0.12098
wandb:   IoU loss 0.32243
wandb:      epoch 200
wandb:       loss 0.45649
wandb:         lr 1e-05
```

# error: src refspec convert_to_ufo does not match any.

```jsx
git checkout convert_to_ufo 한 후에, git push origin convert_to_ufo 해주기 
```

# concatdataset

- 참고 : [torch.utils.data.dataset — PyTorch 1.11.0 documentation](https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#ConcatDataset)

```jsx
class ConcatDataset(Dataset[T_co]):
    r"""Dataset as a concatenation of multiple datasets.

    This class is useful to assemble different existing datasets.

    Args:
        datasets (sequence): List of datasets to be concatenated
    """
    datasets: List[Dataset[T_co]]
    cumulative_sizes: List[int]

    @staticmethod
    def cumsum(sequence):
        r, s = [], 0
        for e in sequence:
            l = len(e)
            r.append(l + s)
            s += l
        return r

    def __init__(self, datasets: Iterable[Dataset]) -> None:
        super(ConcatDataset, self).__init__()
        self.datasets = list(datasets)
        assert len(self.datasets) > 0, 'datasets should not be an empty iterable'  # type: ignore[arg-type]
        for d in self.datasets:
            assert not isinstance(d, IterableDataset), "ConcatDataset does not support IterableDataset"
        self.cumulative_sizes = self.cumsum(self.datasets)

    def __len__(self):
        return self.cumulative_sizes[-1]

    def __getitem__(self, idx):
        if idx < 0:
            if -idx > len(self):
                raise ValueError("absolute value of index should not exceed dataset length")
            idx = len(self) + idx
        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)
        if dataset_idx == 0:
            sample_idx = idx
        else:
            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]
        return self.datasets[dataset_idx][sample_idx]

    @property
    def cummulative_sizes(self):
        warnings.warn("cummulative_sizes attribute is renamed to "
                      "cumulative_sizes", DeprecationWarning, stacklevel=2)
        return self.cumulative_sizes
```

```jsx
# custom 한 코드

import os, warnings
import bisect
from typing import TypeVar
T_co = TypeVar('T_co', covariant=True)
T = TypeVar('T')

class ConcatTextDataset(Dataset[T_co]):

    @staticmethod
    def cumsum(sequence):
        r, s = [], 0
        for e in sequence:
            l = len(e)
            r.append(l + s)
            s += l
        return r
    
    @property
    def cummulative_sizes(self):
        warnings.warn("cummulative_sizes attribute is renamed to "
                      "cumulative_sizes", DeprecationWarning, stacklevel=2)
        return self.cumulative_sizes

    def __init__(self, root_dir, split='train', image_size=1024, crop_size=512, color_jitter=True,
                 normalize=True):

        root_dir_list = []
        folder_list = next(os.walk(root_dir))[1]
        for f in folder_list :
            p = os.path.join(root_dir, f)
            root_dir_list.append(p)
        self.root_dir_list = root_dir_list

        datasets = [SceneTextDataset(path) for path in self.root_dir_list]

        self.datasets = list(datasets)
        assert len(self.datasets) > 0, 'datasets should not be an empty iterable'  # type: ignore[arg-type]
        
        self.cumulative_sizes = self.cumsum(self.datasets)

        anno_list = []
        for root in self.root_dir_list:
            with open(osp.join(root, 'ufo/{}.json'.format(split)), 'r') as f:
                anno = json.load(f)
                anno_list.append(anno)

        self.anno_list = anno_list
        self.image_fnames_list = [sorted(ann['images'].keys()) for ann in self.anno_list]
        self.image_dir_list = [osp.join(root, 'images') for root in self.root_dir_list]

        self.image_size, self.crop_size = image_size, crop_size
        self.color_jitter, self.normalize = color_jitter, normalize

    def __len__(self):
        return self.cumulative_sizes[-1]

    def __getitem__(self, idx):
        # index 
        if idx < 0:
            if -idx > len(self):
                raise ValueError("absolute value of index should not exceed dataset length")
            idx = len(self) + idx

        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)
        if dataset_idx == 0:
            sample_idx = idx
        else:
            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]

        image_fname = self.image_fnames_list[dataset_idx][sample_idx]
        image_fpath = osp.join(self.image_dir_list[dataset_idx], image_fname)

        vertices, labels = [], []
        for word_info in self.anno_list[dataset_idx]['images'][image_fname]['words'].values():
            vertices.append(np.array(word_info['points']).flatten())
            labels.append(int(not word_info['illegibility']))

        print(f"{image_fpath}")

        vertices, labels = np.array(vertices, dtype=np.float32), np.array(labels, dtype=np.int64)

        vertices, labels = filter_vertices(vertices, labels, ignore_under=10, drop_under=1)

        image = Image.open(image_fpath)
        image, vertices = resize_img(image, vertices, self.image_size)
        image, vertices = adjust_height(image, vertices)
        image, vertices = rotate_img(image, vertices)
        image, vertices = crop_img(image, vertices, labels, self.crop_size)

        if image.mode != 'RGB':
            image = image.convert('RGB')
        image = np.array(image)

        funcs = []
        func = A.OneOf([
            # 담당 Augmentation 군 입력
            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, always_apply=False, p=1.0),
            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, brightness_by_max=True, always_apply=False, p=1.0),
            A.RandomGamma(gamma_limit=(80, 120), eps=None, always_apply=False, p=1.0),
            A.ColorJitter(0.5, 0.5, 0.5, 0.25, p=1.0)
        ], p=0.5) # p=0.1
        funcs.append(func)

        funcs.append(A.augmentations.transforms.ChannelShuffle(p=0.5))
        
        if self.normalize:
            funcs.append(A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)))
        
        transform = A.Compose(funcs)
        image = transform(image=image)['image']
        word_bboxes = np.reshape(vertices, (-1, 4, 2))
        roi_mask = generate_roi_mask(image, vertices, labels)

        return image, word_bboxes, roi_mask
```

# ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (8,) + inhomogeneous part.

- 특정 데이터 셋에서만 해당 에러 발생 → 그 데이터 셋만 빼고 concatdataset 진행
